{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Literature-Review\" data-toc-modified-id=\"Literature-Review-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Literature Review</a></span></li><li><span><a href=\"#SchNet-Implementation\" data-toc-modified-id=\"SchNet-Implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>SchNet Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href=\"#SchNet-model\" data-toc-modified-id=\"SchNet-model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>SchNet model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distance-expansion\" data-toc-modified-id=\"Distance-expansion-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Distance expansion</a></span></li><li><span><a href=\"#Continuous-filter-convolutional-layer\" data-toc-modified-id=\"Continuous-filter-convolutional-layer-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Continuous-filter convolutional layer</a></span></li><li><span><a href=\"#Interaction-block\" data-toc-modified-id=\"Interaction-block-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Interaction block</a></span></li><li><span><a href=\"#SchNet-Model\" data-toc-modified-id=\"SchNet-Model-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>SchNet Model</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Invariance-to-permutation\" data-toc-modified-id=\"Invariance-to-permutation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Invariance to permutation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mathematical-analysis\" data-toc-modified-id=\"Mathematical-analysis-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Mathematical analysis</a></span></li><li><span><a href=\"#Experimental-verification\" data-toc-modified-id=\"Experimental-verification-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Experimental verification</a></span></li></ul></li><li><span><a href=\"#Invariance-to-rotation\" data-toc-modified-id=\"Invariance-to-rotation-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Invariance to rotation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mathematical-analysis\" data-toc-modified-id=\"Mathematical-analysis-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Mathematical analysis</a></span></li><li><span><a href=\"#Experimental-verification\" data-toc-modified-id=\"Experimental-verification-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Experimental verification</a></span></li></ul></li><li><span><a href=\"#Main-function\" data-toc-modified-id=\"Main-function-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Main function</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review\n",
    "\n",
    "**[SchNet: A continuous-filter convolutional neural network for modeling quantum interactions](https://arxiv.org/abs/1706.08566)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# SchNet Implementation\n",
    "\n",
    "- Dataset: QM9\n",
    "- Benchmarks: U0 (Internal energy at 0K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from math import pi as PI\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_add_pool, radius_graph, MessagePassing\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "> **Key steps:**\n",
    "> \n",
    "> - Download data and set target;\n",
    "> - Use ```atomref``` to correct the energy of intramolecular interactions;\n",
    "> - Transform the unit from eV to kcal/mol to be consistent with the paper;\n",
    "> - Normalize the data to improve model generalization;\n",
    "> - Split training and test set, use ```50000``` samples for training, ```1000``` for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset = QM9(root='./qm9')\n",
    "\n",
    "    target = 7\n",
    "    KCALMOL2EV = 0.04336414\n",
    "    num_train = 50000\n",
    "    num_test = 1000\n",
    "\n",
    "    atomref = dataset.atomref(target)\n",
    "    for index, data in enumerate(dataset):\n",
    "        dataset[index].y[:, target] = data.y[:, target] - atomref[data.z].sum()\n",
    "\n",
    "    dataset.y = dataset.y // KCALMOL2EV\n",
    "\n",
    "    mean = dataset.y.mean(dim=0, keepdim=True)\n",
    "    std = dataset.y.std(dim=0, keepdim=True)\n",
    "    dataset.y = (dataset.y - mean) / std\n",
    "    mean, std = mean[:, target].item(), std[:, target].item()\n",
    "    print(f\"mean and std values of U0: mean = {mean}, std = {std}\")\n",
    "\n",
    "    train_dataset = dataset[:num_train]\n",
    "    test_dataset = dataset[num_train:num_train + num_test]\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=64,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=64,\n",
    "                             shuffle=False,\n",
    "                             drop_last=True)\n",
    "    return train_loader, test_loader, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## SchNet model\n",
    "\n",
    "\n",
    "![SchNet Structure](./assets/SchNet.png)\n",
    "\n",
    "Hyperparameters:\n",
    "```python\n",
    "hidden_channels: 64    # The hidden embedding size\n",
    "num_interactions = 3   # The number of interaction blocks\n",
    "num_filters = 64       # The number of filters used in the cfconv layer\n",
    "num_gaussians = 300    # The number of gaussians used in RBF\n",
    "cutoff = 30            # The cutoff value for RBF-based distance expansion\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Distance expansion\n",
    "\n",
    "Expand the atomic distance with Gaussian Radial Basis Functions (RBF)  to describe the atomic interactions in the molecule: \n",
    "\n",
    "$$e_k(\\mathbf{r}_i-\\mathbf{r}_j)=\\text{exp}(-\\gamma \\Vert d_{ij}-\\mu_k \\Vert ^2), \\text{where } d_{ij}=\\Vert \\mathbf{r}_i-\\mathbf{r}_j \\Vert.$$\n",
    "\n",
    "In the paper, $0<\\mu_k<30$, and $\\mu_{k+1}-\\mu{k}=0.1$. Therefore, we set the ```cutoff = 30``` and ```num_guassians = 300``` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class GaussianSmearing(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cutoff: float = 30.0,\n",
    "        num_gaussians: int = 300,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        offset = torch.linspace(0.0, cutoff, num_gaussians)\n",
    "        self.coeff = -10\n",
    "        self.register_buffer('offset', offset)\n",
    "\n",
    "    def forward(self, dist: Tensor) -> Tensor:  # dist = d_{ij} range: 0-7\n",
    "        dist = dist.view(-1, 1) - self.offset.view(1, -1)\n",
    "        return torch.exp(self.coeff * torch.pow(dist, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Continuous-filter convolutional layer \n",
    "\n",
    "$$W^l: \\mathbb{R}^D \\rightarrow \\mathbb{R}^F$$\n",
    "\n",
    "$$\\mathbf{x}_i^{l+1}=\\left(X^l * W^l\\right)_i=\\sum_j \\mathbf{x}_j^l \\circ W^l\\left(\\mathbf{r}_i-\\mathbf{r}_j\\right)$$\n",
    "\n",
    "In the paper, the variable $W$ is obtained using an MLP neural network with two fully-connected layers followed with activation function, and the same approach is used here. The ```edge_attr``` is generated with the RBF. And the propagation includes message passing, aggregation, and updating processes. Since the ```aggregate``` and ```update``` functions remains the same as the inherited class, I only re-write the message function according to the cfconv equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CFConv(MessagePassing):\n",
    "\n",
    "    def __init__(self, nn_layers: nn.Sequential):\n",
    "        super().__init__(aggr='add')\n",
    "        self.nn = nn_layers\n",
    "\n",
    "    def forward(self, h, edge_index, edge_weight, edge_attr):\n",
    "        W = self.nn(edge_attr)\n",
    "        x = self.propagate(edge_index, x=h,W=W)  # message -> aggregate -> update\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j: Tensor, W: Tensor) -> Tensor:\n",
    "        return x_j * W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Interaction block\n",
    "\n",
    "The interaction block includes an atom-wise layer, a cfconv layer, and an MLP, responsible for updating the atomic representations based on the molecular geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Interaction(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_channels: int, num_gaussians: int,\n",
    "                 num_filters: int):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_gaussians, hidden_channels),\n",
    "            ShiftedSoftplus(),\n",
    "            nn.Linear(hidden_channels, num_filters),\n",
    "            ShiftedSoftplus(),\n",
    "        )\n",
    "        self.atom_wise = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.conv = CFConv(self.mlp)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            ShiftedSoftplus(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.atom_wise.weight)\n",
    "        self.atom_wise.bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.mlp[0].weight)\n",
    "        self.mlp[0].bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.mlp[2].weight)\n",
    "        self.mlp[2].bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.out[0].weight)\n",
    "        self.out[0].bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.out[2].weight)\n",
    "        self.out[2].bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, h, edge_index, edge_weight, edge_attr):\n",
    "        h = self.atom_wise(h)\n",
    "        h = self.conv(h, edge_index, edge_weight, edge_attr)\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### SchNet Model\n",
    "\n",
    "SchNet is composed of an embedding layer, several interaction layers (3 in the paper), one atom-wise layer followed with activation function and one atom-wise layer followed with sum pooling layer. Here the number of hidden channels is set to 64 as the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SchNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SchNetModel, self).__init__()\n",
    "\n",
    "        self.hidden_channels = 64\n",
    "        self.num_interactions = 3\n",
    "        self.num_filters = 64\n",
    "        self.num_gaussians = 300\n",
    "        self.cutoff = 10.0\n",
    "\n",
    "        self.embedding = nn.Embedding(100, self.hidden_channels, padding_idx=0)\n",
    "        self.interactions = nn.ModuleList()\n",
    "        for _ in range(self.num_interactions):\n",
    "            block = Interaction(self.hidden_channels, self.num_gaussians,\n",
    "                                self.num_filters)\n",
    "            self.interactions.append(block)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.hidden_channels, self.hidden_channels // 2),\n",
    "            ShiftedSoftplus(), nn.Linear(self.hidden_channels // 2, 1))\n",
    "        self.distance_expansion = GaussianSmearing(self.cutoff,\n",
    "                                                   self.num_gaussians)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "        for interaction in self.interactions:\n",
    "            interaction.reset_parameters()\n",
    "        torch.nn.init.xavier_uniform_(self.layers[0].weight)\n",
    "        self.layers[0].bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.layers[2].weight)\n",
    "        self.layers[2].bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, z, batch = data.pos, data.z, data.batch\n",
    "        pos.require_grad = True\n",
    "\n",
    "        edge_index = radius_graph(pos, r=self.cutoff, batch=batch)\n",
    "        row, col = edge_index\n",
    "        edge_weight = (pos[row] - pos[col]).norm(dim=-1)\n",
    "        edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "        x_emb = self.embedding(z)\n",
    "        h = x_emb\n",
    "        for interaction in self.interactions:\n",
    "            h = h + interaction(h, edge_index, edge_weight, edge_attr)\n",
    "        h = self.layers(h)\n",
    "        out = global_add_pool(h, batch)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ShiftedSoftplus(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shift = torch.log(torch.tensor(2.0)).item()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.softplus(x) - self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Report loss and Mean Absolute Error (MAE) after each epoch, where the loss is calculated with normalized target and the mae is calculated with the real U0 values restored using ```mean``` and ```std``` in data preparation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, epoch, loader, device, mean, std):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    for data in loader:\n",
    "        data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            target = data.y[:, 7].unsqueeze(1)\n",
    "            loss = F.mse_loss(out, target)\n",
    "\n",
    "            out = out * std + mean\n",
    "            target = target * std + mean\n",
    "            mae = torch.sum(torch.abs(target - out)) // len(target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "    loss = total_loss / len(loader)\n",
    "    mae = total_mae / len(loader)\n",
    "    print(f\"Validation Epoch {epoch+1} Loss = {loss:.6f}, MAE = {mae:.6f}\")\n",
    "    return loss, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Invariance to permutation\n",
    "\n",
    "### Mathematical analysis\n",
    "\n",
    "Permutation invariance means that the output of the network does not change if the order of the input atoms is changed. In SchNet, permutation invariance is achieved through its use of **continuous-filter convolutional layers** and the way it handles atomic features.Other layers such as **dense layers** and **atom-wise layers** do not inherently contribute to this symmetry property because they do not handle atomic interactions.\n",
    "\n",
    "- In SchNet, a molecule in a certain conformation can be described uniquely by a set of $n$ atoms with nuclear charges $\\mathbf{Z}=(\\mathbf{Z}_1, ...,\\mathbf{Z}_n)$ and atomic positions $\\mathbf{R}=(\\mathbf{r}_1, ...,\\mathbf{r}_n)$. To obtain a standard graph $\\mathbf{G}$ with a set of nodes $\\mathbf{V}$ and edges $\\mathbf{E}$, $\\mathbf{Z}$ is embedded into $\\mathbf{V} \\in \\mathbb{R}^F$, where $F$ is the dimension of hidden features. Adjancency matrix $\\mathbf{E}$ is generated by connecting every atom to every atom, which makes it invariant to atom order and $\\mathbf{G}$ a complete graph. Consider a permutation $\\sigma$ on the atom order, we have:\n",
    "\n",
    "$$\\mathbf{E}(\\mathbf{V}_1, ...,\\mathbf{V}_n) = \\mathbf{E}(\\mathbf{V}_{\\sigma(1)}, ...,\\mathbf{V}_{\\sigma(n)}) = \\begin{bmatrix}\n",
    " 0 & 1 & ... & 1\\\\\n",
    " 1 & 0 & ... & 1\\\\\n",
    " \\vdots & \\vdots &\\ddots &\\vdots \\\\\n",
    " 1 & 1 & ... & 0 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "- Moreover, in SchNet, features of atoms are updated based on interactions with neighboring atoms. These interactions are aggregated using a sum over neighbors, which is a permutation-invariant operation. Consider a set of atom features ${\\mathbf{h}_1,...,\\mathbf{h}_n}$ and a permutation $\\sigma$ of these features ${\\mathbf{h}_{\\sigma(1)},...,\\mathbf{h}_{\\sigma(n)}}$. The update of an atom's feature in SchNet depends on the sum of features of neighboring atoms. If we permute the atoms, the summation operation in the aggregation phase remains unchanged because summing is invariant to the order of its operands:\n",
    "\n",
    "$$\\text{sum}(\\mathbf{h}_1,...,\\mathbf{h}_n)=\\text{sum}(\\mathbf{h}_{\\sigma(1)},...,\\mathbf{h}_{\\sigma(n)}) $$\n",
    "\n",
    "- Another reason for SchNet to be invariant to permutations is that SchNet generates filters dynamically based on the distances between atoms. These filters are used to modulate the influence of neighboring atoms' features based on their relative distances $\\Vert \\mathbf{r}_i - \\mathbf{r}_j \\Vert$. Since the generation and application of these filters are solely based on distances (which are permutation invariant), the output of the convolutional layers remains invariant to permutations of the input atoms. \n",
    "\n",
    "Conclusively, the primary reasons why SchNet is invariant to permutations include: 1. Invariant adjacency matrix; 2. Aggregation mechanism; 3. Distance-based filter generation.\n",
    "\n",
    "\n",
    "### Experimental verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def verify_permutation_invariance(model, data):\n",
    "    permuted_data = data.clone()\n",
    "    node_permutation = torch.randperm(data.num_nodes)\n",
    "    permuted_data.x = data.x[node_permutation]\n",
    "    if data.edge_index is not None:\n",
    "        edge_index_remap = {\n",
    "            i: node_permutation[i].item()\n",
    "            for i in range(data.num_nodes)\n",
    "        }\n",
    "        permuted_data.edge_index = torch.tensor(\n",
    "            [[edge_index_remap[i.item()] for i in row]\n",
    "             for row in data.edge_index.t()]).t()\n",
    "\n",
    "    return torch.allclose(model(data), model(permuted_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Invariance to rotation\n",
    "\n",
    "### Mathematical analysis\n",
    "\n",
    "Rotation invariance means the model's output remains unchanged under rotations of the input molecular structure. The key point for SchNet to be rotation-invariant is the usage of interatomic distances $\\Vert \\mathbf{r}_i - \\mathbf{r}_j \\Vert$ as part of its input features.\n",
    "\n",
    "Let $\\mathbf{R}$ be a rotation matrix, and consider the position vectors $\\mathbf{r}_i$ and $\\mathbf{r}_j$ of two atoms. Under rotation, their positions change to $\\mathbf{Rr}_i$ and $\\mathbf{Rr}_j$. The distance between these atoms is:\n",
    "\n",
    "$$\\Vert \\mathbf{Rr}_i - \\mathbf{Rr}_j \\Vert = \\Vert \\mathbf{R}(\\mathbf{r}_i - \\mathbf{r}_j) \\Vert$$\n",
    "\n",
    "Since rotation matrices preserve distances (norms), the above expression simplifies to:\n",
    "\n",
    "$$\\Vert \\mathbf{r}_i - \\mathbf{r}_j \\Vert$$\n",
    "showing that the interatomic distance remains unchanged under rotation.\n",
    "\n",
    "The filters in SchNet, which operate on these distances, therefore, only process rotation-invariant information.\n",
    "\n",
    "### Experimental verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def rotate_molecule(coordinates, angle, axis):\n",
    "    R = torch.eye(3)\n",
    "    angle = torch.tensor(angle)\n",
    "    c, s = torch.cos(angle), torch.sin(angle)\n",
    "    if axis == 0:  # Rotate around x-axis\n",
    "        R[1, 1], R[1, 2], R[2, 1], R[2, 2] = c, -s, s, c\n",
    "    elif axis == 1:  # Rotate around y-axis\n",
    "        R[0, 0], R[0, 2], R[2, 0], R[2, 2] = c, s, -s, c\n",
    "    elif axis == 2:  # Rotate around z-axis\n",
    "        R[0, 0], R[0, 1], R[1, 0], R[1, 1] = c, -s, s, c\n",
    "    return torch.matmul(coordinates, R)\n",
    "\n",
    "\n",
    "def verify_rotation_invariance(model, data):\n",
    "    rotated_data = data.clone()\n",
    "    angle = np.random.uniform(0, 2 * np.pi)\n",
    "    axis = np.random.choice([0, 1, 2])\n",
    "    rotated_data.pos = rotate_molecule(data.pos, angle, axis)\n",
    "\n",
    "    return torch.allclose(model(data), model(rotated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = SchNetModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=0.96,\n",
    "                                                   last_epoch=-1)\n",
    "\n",
    "pretrained = True\n",
    "model_path = 'schnet_energy_model_100.pth'\n",
    "dataset = QM9(root='./qm9')\n",
    "train_loader, test_loader, mean, std = load_data()\n",
    "\n",
    "data = train_loader.dataset[0]\n",
    "print(\"All attributes that QM9 molecular graph contains:\")\n",
    "print(data)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "if not pretrained:\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        total_mae = 0\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            target = data.y[:, 7].unsqueeze(1)\n",
    "\n",
    "            loss = F.mse_loss(\n",
    "                out, target)  # Using the 7th property U0 as the target\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            out = out * std + mean\n",
    "            target = target * std + mean\n",
    "            mae = torch.sum(torch.abs(target - out)) // len(target)\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 10000 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "        mse = total_loss / len(train_loader)\n",
    "        mae = total_mae / len(train_loader)\n",
    "        print(\n",
    "            f\"Training {epoch} Loss = {loss:.6f}, MAE = {mae:.6f}, LR = {scheduler.get_last_lr().item()}\"\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        loss, test_mae = evaluate_model(model, epoch, test_loader, device,\n",
    "                                        mean, std)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1} Model Saving ...\")\n",
    "            torch.save(model.state_dict(),\n",
    "                       f\"schnet_energy_model_{epoch+1}.pth\")\n",
    "else:\n",
    "    model = SchNetModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "sample_data = next(iter(test_loader))\n",
    "print(\"Permutation Invariance:\", verify_permutation_invariance(model, sample_data))\n",
    "print(\"Rotation Invariance:\", verify_rotation_invariance(model, sample_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.85px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
